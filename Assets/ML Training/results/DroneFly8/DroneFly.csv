Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
1965000,3.4189377,192.23076923076923,0.87212354,1.0,1.0,0.9846571,0.24467553,5.4404236e-06,0.10181344,1.8885863e-05,1.0
1970000,3.4189384,125.3076923076923,0.7086017,0.9487179487179487,0.9487179487179487,0.7514025,0.23465557,4.8740976e-06,0.10162467,1.7960865e-05,1.0
1975000,3.4189382,205.2,0.6540742,1.0,1.0,0.61559224,0.2415762,4.1292456e-06,0.10137638,1.6744269e-05,1.0
1980000,3.4189384,230.0,0.5355988,0.9545454545454546,0.9545454545454546,0.5179944,0.24858022,3.3730964e-06,0.10112433,1.550923e-05,1.0
1985000,3.4189382,431.9166666666667,0.3428416,1.0,1.0,0.53160334,0.24243619,2.6205723e-06,0.10087349,1.4280104e-05,1.0
1990000,3.4189384,216.69565217391303,0.39568332,1.0,1.0,0.4218216,0.24191295,1.8735229e-06,0.10062448,1.3059925e-05,1.0
1995000,3.4189384,616.5555555555555,0.3916103,1.0,1.0,0.39366683,0.23979938,1.1248152e-06,0.10037491,1.1837035e-05,1.0
2000000,3.4189382,195.0,0.37180987,0.9565217391304348,0.9565217391304348,0.40792796,0.2422965,3.8246858e-07,0.10012746,1.0624535e-05,1.0
